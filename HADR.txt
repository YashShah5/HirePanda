ğŸ§­ Unified Approach Outline for PDLMGIT-308

Goal: Ensure MongoDB health is monitored and auditable, with HA/DR and alerting integrated into Datadog / Splunk for our critical services.
1. ğŸ§‘â€ğŸ’¼ Opening (for either mentor or DBA)

â€œIâ€™ve taken ownership of PDLMGIT-308, and my focus is on making sure MongoDBâ€™s availability is not a blind spot. Iâ€™ve identified 3 critical areas: HA/DR setup, server health alerting, and operational logging. Iâ€™m coordinating with both the DBA and internal teams to move this forward.â€
2. ğŸ¯ Problem Framing

Break the story into these sub-problems:
A. HA/DR

    Goal: Ensure there is a fallback if the primary MongoDB server fails.

    Exploratory Questions:

        â€œDo we currently have MongoDB HA or DR set up?â€

        â€œIf not, what steps are required to enable that?â€

        â€œDo we need to set up replication or clusters?â€

        â€œWhat recovery time objective (RTO) and recovery point objective (RPO) are realistic here?â€

B. Alerting

    Goal: Receive alerts if MongoDB becomes inaccessible.

    Exploratory Questions:

        â€œCan the DBA team expose health/status metrics (CPU, memory, availability) to us?â€

        â€œWhatâ€™s the existing process for integrating those into Datadog?â€

        â€œDo we need to configure custom YAMLs for scraping metrics?â€

        â€œWould this be similar to whatâ€™s done for GitLab servers?â€

C. Logging for Auditing

    Goal: Log important operations (e.g., update/delete) for audit trails.

    Exploratory Questions:

        â€œCan these logs be captured at the DB server level?â€

        â€œCan they be pushed to Datadog or Splunk?â€

        â€œAre logs retained anywhere today by default?â€

3. ğŸ¤ Collaboration & Ownership

Your Role:

    Confirm requirements

    Coordinate across teams (DBA, Datadog, Electron, etc.)

    Raise tickets as needed

    Ensure follow-up happens and gaps are escalated

What You Need from Others:

    DBA: Server setup, monitoring config, feasibility answers

    Electron/Monitoring team: YAML guidance, dashboard support
-------------------------------------------------------------------------

ğŸ§¾ Summary: Meeting with Sourajit (Mentor/Guide)
ğŸ¯ Main Objective

To guide you on how to handle PDLMGIT-308: setting up MongoDB health monitoring, alerting, and logging, by collaborating with the DBA team.
ğŸ§© Key Points Discussed
1. MongoDB Is Critical

    The MongoDB servers are Crit 1 for your teamâ€™s tools.

    If they go down, you must be alerted immediately.

2. Whatâ€™s Needed

    HA/DR setup (to handle failovers).

    Health Alerting if the DB or infra becomes inaccessible.

    Operational Logging (e.g., update/delete queries) to Datadog or Splunk.

3. DBA Team Involvement

    The DBA team already set up something similar for GitLab, so they should be able to do it for MongoDB too.

    Monitoring strings will be provided by the DBA team â€” your team (or Electron) can plug them into Datadog.

4. Your Responsibilities

    You are not building alerting, but you are:

        Gathering requirements

        Initiating collaboration

        Making sure the pieces are in place (HA/DR, alerting, logs)

ğŸªœ Next Steps You Were Told to Take
âœ… Step 1: Confirm Server Details

    Talk to Alex or Akshi during your sync to confirm which MongoDB servers are PROD and TEST.

âœ… Step 2: Reach Out to DBA Night Contact (Gantlapu)

    Introduce yourself, say youâ€™re from the GitHub team.

    Tell him these servers are Crit 1 and you need:

        HA/DR

        Alerting setup

        Logging for audits

    Ask if they already have anything in place and what they recommend.

    Ask if a ticket is needed, and who to tag for follow-ups.

âœ… Step 3: Team Communication

    Start a Teams thread in your GitHub channel:

        Mention this is for PDLMGIT-308.

        Document every interaction with the DBA team.

        Share status, blockers, and findings.

        Tag Vasu or others for support if needed.

ğŸ’¬ Extra Notes

    Your story is related but separate from the one for GitHub Admin VM monitoring.

    That one is simpler because your team owns the VM. For MongoDB, DBA owns it â€” so collaboration is required.

    A past outage incident led to this story â€” because MongoDB wasnâ€™t monitored, it stayed down unnoticed.


Purpose: Collaborate with the DBA team to ensure monitoring, alerting, and logging are in place for our critical MongoDB servers used by PDLM tools.
ğŸ” Background & Context

    This effort was initiated following an incident where our MongoDB servers went down unnoticed due to lack of monitoring.

    The MongoDB servers are Crit 1 â€” essential for the PDLM toolset.

    We are seeking DBA support to implement:

        High Availability / Disaster Recovery (HA/DR)

        Health Monitoring / Alerting

        Operational Logging (e.g., update/delete queries)

ğŸ§© Objectives of the Call

    Confirm whether HA/DR is currently configured

    Understand existing monitoring/alerting setup (if any)

    Discuss feasibility of integrating alerts into Datadog

    Explore options for logging queries to Datadog or Splunk

    Clarify ticketing process and who to tag for future steps

â“Discussion Topics & Questions
ğŸ›¡ï¸ HA/DR

    Are the MongoDB PROD/TEST servers configured with High Availability or DR?

    If not, can you advise on how to proceed?

    What would the failover process look like today?

ğŸš¨ Monitoring / Alerting

    Is there any existing health alerting configured (similar to GitLab)?

    Can alerts be exposed or forwarded to be used with Datadog?

    What configuration or YAML would be required on our side (Electron can help if needed)?

ğŸ“‹ Logging / Auditing

    Can query operations (especially update/delete) be logged?

    Is your team able to forward these logs to Datadog or Splunk?

    If this isn't done already, what would it take to enable this?

ğŸ§¾ Ticketing & Process

    Once the requirements are confirmed, should I raise a Jira ticket?

    Any formatting or fields youâ€™d like included?

    Who on the DBA team should I tag for continued support?

ğŸ“Œ Internal Coordination

Confirm MongoDB server names (PROD/TEST) with Alex or Akshi

Validate metrics/logging support format with Electron (Datadog side)

    Create Teams thread to document and share progress with the broader team

ğŸªœ Next Steps (Post-Call)

    Document Gantlapu's responses in the Teams thread

    Raise Jira ticket based on DBA team's feedback

    Begin wiring metrics/logs into Datadog/Splunk (if support is provided)

    Escalate blockers via Vasu if needed

